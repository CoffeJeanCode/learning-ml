{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f7c5ca",
   "metadata": {},
   "source": [
    "# Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421f8c3",
   "metadata": {},
   "source": [
    "## Derivates\n",
    "- Measure change ratio of a function\n",
    "- Use for optimization finding min/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b62ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "105143c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\left(\\left(2 \\operatorname{re}{\\left(x\\right)} \\frac{d}{d x} \\operatorname{re}{\\left(x\\right)} - 2 \\operatorname{im}{\\left(x\\right)} \\frac{d}{d x} \\operatorname{im}{\\left(x\\right)}\\right) \\left(\\left(\\operatorname{re}{\\left(x\\right)}\\right)^{2} - \\left(\\operatorname{im}{\\left(x\\right)}\\right)^{2}\\right) + 2 \\left(2 \\operatorname{re}{\\left(x\\right)} \\frac{d}{d x} \\operatorname{im}{\\left(x\\right)} + 2 \\operatorname{im}{\\left(x\\right)} \\frac{d}{d x} \\operatorname{re}{\\left(x\\right)}\\right) \\operatorname{re}{\\left(x\\right)} \\operatorname{im}{\\left(x\\right)}\\right) \\operatorname{sign}{\\left(x^{2} \\right)}}{x^{2}}$"
      ],
      "text/plain": [
       "((2*re(x)*Derivative(re(x), x) - 2*im(x)*Derivative(im(x), x))*(re(x)**2 - im(x)**2) + 2*(2*re(x)*Derivative(im(x), x) + 2*im(x)*Derivative(re(x), x))*re(x)*im(x))*sign(x**2)/x**2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sp.Symbol(\"x\")\n",
    "f = abs(x**2)\n",
    "\n",
    "derivate = sp.diff(f, x)\n",
    "derivate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcb3b9",
   "metadata": {},
   "source": [
    "### Partial Derivates\n",
    "- Measure the ratio with respect only one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac6c607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*x\n",
      "5*y**4\n"
     ]
    }
   ],
   "source": [
    "x, y = sp.symbols(\"x y\")\n",
    "f = x**2 + y**5\n",
    "\n",
    "partial_x = sp.diff(f, x)\n",
    "partial_y = sp.diff(f, y)\n",
    "print(partial_x)\n",
    "print(partial_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66cd49",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "- Vector of all partial derivates, indicating the direction of the steepest ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fadf0080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/y\n",
      "-x/y**2\n"
     ]
    }
   ],
   "source": [
    "x, y = sp.symbols(\"x y\")\n",
    "f = x/y\n",
    "\n",
    "grad_x = sp.diff(f, x)\n",
    "grad_y = sp.diff(f, y)\n",
    "print(grad_x)\n",
    "print(grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e39a89",
   "metadata": {},
   "source": [
    "## Gradient Descent Optimization Algorithm\n",
    "- Is a iterative optimization algorithm used to find the minimum\n",
    "- Update rule: $\\theta = \\theta - \\alpha \\bigtriangledown f(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777fd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta, learning_rate, iterations):\n",
    "  m = len(y)\n",
    "\n",
    "  for _ in range(iterations):\n",
    "    predictions = np.dot(x, theta)\n",
    "    errors = predictions - y\n",
    "    gradients = (1/m) * np.dot(x.T, errors)\n",
    "    theta = theta - learning_rate * gradients\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae38c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05349176  0.11139888  0.27628953]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([1, 2])\n",
    "\n",
    "theta = np.array([0.1, 0.1, 0.1])\n",
    "learning_rate = 0.01\n",
    "iterations = 1500\n",
    "\n",
    "optimized_theta = gradient_descent(x, y, theta, learning_rate, iterations)\n",
    "print(optimized_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3799fb",
   "metadata": {},
   "source": [
    "## Integrals\n",
    "- Compute the are under a curve, representing accumulations\n",
    "- Use for\n",
    "  - Probability distribution\n",
    "  - Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b592bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6\n",
      "-x**3/3 + x**2/2\n"
     ]
    }
   ],
   "source": [
    "x = sp.Symbol(\"x\")\n",
    "f = x - x**2\n",
    "\n",
    "definite_integral = sp.integrate(f, (x, 0, 1))\n",
    "indefinite_integral = sp.integrate(f, x)\n",
    "print(definite_integral)\n",
    "print(indefinite_integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e267964",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "- Optimization algorithm that use random subsets of the data to compute gradients and update parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf91481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(x, y, theta, learning_rate, n_epocs):\n",
    "  m = len(y)\n",
    "\n",
    "  for epoch in range(n_epocs):\n",
    "    for i in range(m):\n",
    "      random_index = np.random.randint(m)\n",
    "      x1 = x[random_index:random_index + 1]\n",
    "      y1 = y[random_index:random_index + 1]\n",
    "      predictions = np.dot(x1, theta)\n",
    "      errors = predictions - y1\n",
    "      gradients = 2 / m * np.dot(x1.T, errors)\n",
    "\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d6f841b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7318976 ],\n",
       "       [-0.96643019]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(41)\n",
    "x = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * x + np.random.randn(100, 1)\n",
    "\n",
    "x_b = np.c_[np.ones((100, 1)), x]\n",
    "theta = np.random.randn(2, 1)\n",
    "learning_rate = 0.01\n",
    "n_epocs = 500\n",
    "\n",
    "optimized_theta = stochastic_gradient_descent(x_b, y, theta, learning_rate, n_epocs)\n",
    "optimized_theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
